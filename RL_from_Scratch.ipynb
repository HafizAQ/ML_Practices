{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCr0ZDWjnnzRDprB6U0Nqo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HafizAQ/ML_Practices/blob/main/RL_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Courtesy Reference: https://medium.com/@wangdk93/reinforcement-learning-from-scratch-ee9b7218e70d\n",
        "\n",
        "#Reinforcement Learning from Scratch"
      ],
      "metadata": {
        "id": "dSkVZ32Ev28t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LSJYveb7usnc",
        "outputId": "eea1853a-e998-411b-b90f-b2364ee556cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "Successfully installed matplotlib-3.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show matplotlib"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETmCCGpYu3wT",
        "outputId": "f008122d-da30-4eb7-d26a-cefe5d5428d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: matplotlib\n",
            "Version: 3.9.3\n",
            "Summary: Python plotting package\n",
            "Home-page: https://matplotlib.org\n",
            "Author: John D. Hunter, Michael Droettboom\n",
            "Author-email: Unknown <matplotlib-users@python.org>\n",
            "License: License agreement for matplotlib versions 1.3.0 and later\n",
            "         =========================================================\n",
            "         \n",
            "         1. This LICENSE AGREEMENT is between the Matplotlib Development Team\n",
            "         (\"MDT\"), and the Individual or Organization (\"Licensee\") accessing and\n",
            "         otherwise using matplotlib software in source or binary form and its\n",
            "         associated documentation.\n",
            "         \n",
            "         2. Subject to the terms and conditions of this License Agreement, MDT\n",
            "         hereby grants Licensee a nonexclusive, royalty-free, world-wide license\n",
            "         to reproduce, analyze, test, perform and/or display publicly, prepare\n",
            "         derivative works, distribute, and otherwise use matplotlib\n",
            "         alone or in any derivative version, provided, however, that MDT's\n",
            "         License Agreement and MDT's notice of copyright, i.e., \"Copyright (c)\n",
            "         2012- Matplotlib Development Team; All Rights Reserved\" are retained in\n",
            "         matplotlib  alone or in any derivative version prepared by\n",
            "         Licensee.\n",
            "         \n",
            "         3. In the event Licensee prepares a derivative work that is based on or\n",
            "         incorporates matplotlib or any part thereof, and wants to\n",
            "         make the derivative work available to others as provided herein, then\n",
            "         Licensee hereby agrees to include in any such work a brief summary of\n",
            "         the changes made to matplotlib .\n",
            "         \n",
            "         4. MDT is making matplotlib available to Licensee on an \"AS\n",
            "         IS\" basis.  MDT MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\n",
            "         IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, MDT MAKES NO AND\n",
            "         DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\n",
            "         FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF MATPLOTLIB\n",
            "         WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n",
            "         \n",
            "         5. MDT SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF MATPLOTLIB\n",
            "          FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR\n",
            "         LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING\n",
            "         MATPLOTLIB , OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF\n",
            "         THE POSSIBILITY THEREOF.\n",
            "         \n",
            "         6. This License Agreement will automatically terminate upon a material\n",
            "         breach of its terms and conditions.\n",
            "         \n",
            "         7. Nothing in this License Agreement shall be deemed to create any\n",
            "         relationship of agency, partnership, or joint venture between MDT and\n",
            "         Licensee.  This License Agreement does not grant permission to use MDT\n",
            "         trademarks or trade name in a trademark sense to endorse or promote\n",
            "         products or services of Licensee, or any third party.\n",
            "         \n",
            "         8. By copying, installing or otherwise using matplotlib ,\n",
            "         Licensee agrees to be bound by the terms and conditions of this License\n",
            "         Agreement.\n",
            "         \n",
            "         License agreement for matplotlib versions prior to 1.3.0\n",
            "         ========================================================\n",
            "         \n",
            "         1. This LICENSE AGREEMENT is between John D. Hunter (\"JDH\"), and the\n",
            "         Individual or Organization (\"Licensee\") accessing and otherwise using\n",
            "         matplotlib software in source or binary form and its associated\n",
            "         documentation.\n",
            "         \n",
            "         2. Subject to the terms and conditions of this License Agreement, JDH\n",
            "         hereby grants Licensee a nonexclusive, royalty-free, world-wide license\n",
            "         to reproduce, analyze, test, perform and/or display publicly, prepare\n",
            "         derivative works, distribute, and otherwise use matplotlib\n",
            "         alone or in any derivative version, provided, however, that JDH's\n",
            "         License Agreement and JDH's notice of copyright, i.e., \"Copyright (c)\n",
            "         2002-2011 John D. Hunter; All Rights Reserved\" are retained in\n",
            "         matplotlib  alone or in any derivative version prepared by\n",
            "         Licensee.\n",
            "         \n",
            "         3. In the event Licensee prepares a derivative work that is based on or\n",
            "         incorporates matplotlib  or any part thereof, and wants to\n",
            "         make the derivative work available to others as provided herein, then\n",
            "         Licensee hereby agrees to include in any such work a brief summary of\n",
            "         the changes made to matplotlib.\n",
            "         \n",
            "         4. JDH is making matplotlib  available to Licensee on an \"AS\n",
            "         IS\" basis.  JDH MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\n",
            "         IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, JDH MAKES NO AND\n",
            "         DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\n",
            "         FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF MATPLOTLIB\n",
            "         WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n",
            "         \n",
            "         5. JDH SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF MATPLOTLIB\n",
            "          FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR\n",
            "         LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING\n",
            "         MATPLOTLIB , OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF\n",
            "         THE POSSIBILITY THEREOF.\n",
            "         \n",
            "         6. This License Agreement will automatically terminate upon a material\n",
            "         breach of its terms and conditions.\n",
            "         \n",
            "         7. Nothing in this License Agreement shall be deemed to create any\n",
            "         relationship of agency, partnership, or joint venture between JDH and\n",
            "         Licensee.  This License Agreement does not grant permission to use JDH\n",
            "         trademarks or trade name in a trademark sense to endorse or promote\n",
            "         products or services of Licensee, or any third party.\n",
            "         \n",
            "         8. By copying, installing or otherwise using matplotlib,\n",
            "         Licensee agrees to be bound by the terms and conditions of this License\n",
            "         Agreement.\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: contourpy, cycler, fonttools, kiwisolver, numpy, packaging, pillow, pyparsing, python-dateutil\n",
            "Required-by: arviz, bigframes, datascience, fastai, geemap, imgaug, matplotlib-venn, missingno, mlxtend, music21, plotnine, prophet, pycocotools, seaborn, wordcloud, yellowbrick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPBrboxEq6PA",
        "outputId": "3290124c-eca4-49b0-97c9-32496de98306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 0\n",
            "action 0\n",
            "Loss -19666\n",
            "Episode 0, Total Reward: -444.11267070088417\n",
            "episode 1\n",
            "action 1\n",
            "Loss 16761\n",
            "episode 2\n",
            "action 0\n",
            "Loss 37112\n",
            "episode 3\n",
            "action 0\n",
            "Loss 1947\n",
            "episode 4\n",
            "action 2\n",
            "Loss 101\n",
            "episode 5\n",
            "action 2\n",
            "Loss 16574\n",
            "episode 6\n",
            "action 2\n",
            "Loss -35\n",
            "episode 7\n",
            "action 0\n",
            "Loss 8956\n",
            "episode 8\n",
            "action 0\n",
            "Loss 3273\n",
            "episode 9\n",
            "action 2\n",
            "Loss -81\n",
            "episode 10\n",
            "action 0\n",
            "Loss 749\n",
            "episode 11\n",
            "action 0\n",
            "Loss 2886\n",
            "episode 12\n"
          ]
        }
      ],
      "source": [
        "# from matplotlib.dates import epoch2num\n",
        "from matplotlib.dates import num2date\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "from torch.distributions import Categorical  # Import Categorical\n",
        "\n",
        "bounds = [100, 100]  # Map size\n",
        "\n",
        "\n",
        "def generate_simulation_data(num_samples):\n",
        "    data = []\n",
        "    bounds = [100, 100]  # Map size\n",
        "    rock_radius = 5  # radius of stone\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        rock_position = np.random.uniform(rock_radius, bounds[0] - rock_radius, size=2) #Make sure that the starting position of the car is at least a certain distance from the stone. Here we set it to twice the radius of the stone\n",
        "        min_distance = rock_radius * 2\n",
        "        car_position = np.random.uniform(0, bounds, size=2)\n",
        "        while np.linalg.norm(car_position - rock_position) < min_distance:\n",
        "            car_position = np.random.uniform(0, bounds, size=2) # Each sample includes cart location and stone location\n",
        "        data.append((car_position, rock_position))\n",
        "    return data\n",
        "\n",
        "\n",
        "# Generate simulation data\n",
        "num_samples = 1000\n",
        "simulation_data = generate_simulation_data(num_samples)\n",
        "\n",
        "\n",
        "# %%\n",
        "class CarRockEnv:\n",
        "\n",
        "    def __init__(self,\n",
        "                 rock_position,\n",
        "                 rock_radius,\n",
        "                 bounds=bounds,\n",
        "                 max_steps=100,\n",
        "                 number_of_rocks=20):\n",
        "        self.rock_position = np.array(rock_position)\n",
        "        self.rock_radius = rock_radius\n",
        "        self.max_steps = max_steps\n",
        "        self.step_count = 0\n",
        "        self.number_of_rocks = number_of_rocks\n",
        "        self.bounds = bounds\n",
        "        self.move_map = {0: [1, 0], 1: [-1, 0], 2: [0, 1], 3: [0, -1]}\n",
        "        # Initialization code here\n",
        "\n",
        "    def reset(self):\n",
        "        # Place multiple rocks randomly in the environment\n",
        "        self.rocks = [np.random.uniform(0, 100, size=2) for _ in range(self.number_of_rocks)]\n",
        "        self.car_position = np.array([0, 0])  # Reset car position\n",
        "        return self.state()\n",
        "\n",
        "    def state(self):\n",
        "        return np.concatenate((self.car_position / self.bounds, self.rock_position / self.bounds))\n",
        "\n",
        "    def step(self, action):\n",
        "        # Apply action to update car position\n",
        "        self.car_position += self.move_map[action]\n",
        "        self.car_position = np.clip(self.car_position, [0, 0], self.bounds)\n",
        "        reward = -0.1  # Small penalty for taking a step, to encourage efficiency\n",
        "\n",
        "        # Determine the minimum distance to any rock\n",
        "        min_distance = min(np.linalg.norm(self.car_position - rock) for rock in self.rocks)\n",
        "        max_safe_distance = 30  # Define a max safe distance threshold\n",
        "\n",
        "        # Check collision and distance penalties\n",
        "        if min_distance < self.rock_radius:\n",
        "            reward -= 10  # Significant penalty for collision\n",
        "            done = True  # End the episode on collision\n",
        "        elif min_distance > max_safe_distance:\n",
        "            reward -= 5  # Penalty for being too far from the nearest rock\n",
        "            done = False  # Continue the episode\n",
        "        else:\n",
        "            # Optionally reward being within a safe range\n",
        "            reward += 1 - (min_distance / max_safe_distance)  # Scale reward based on closeness\n",
        "            done = False\n",
        "\n",
        "        return self.state(), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        # Optional: Implement rendering to visualize the environment\n",
        "        pass\n",
        "\n",
        "\n",
        "# %%\n",
        "class PolicyNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim))\n",
        "        self.softmax = nn.Softmax(dim = -1)\n",
        "\n",
        "    def forward(self, status):\n",
        "        output = self.network(status)\n",
        "        a = self.softmax(output)\n",
        "        return a\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "def compute_discounted_rewards(rewards, gamma=0.99):\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        discounted_rewards.insert(0, R)\n",
        "    return discounted_rewards\n",
        "\n",
        "\n",
        "def evaluate_policy(policy_net, env, episodes=10):\n",
        "    total_rewards = 0\n",
        "    for i in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action_probs = policy_net(state_tensor)\n",
        "            action = torch.argmax(action_probs).item()\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "        total_rewards += episode_reward\n",
        "\n",
        "    average_reward = total_rewards / episodes\n",
        "    return average_reward\n",
        "\n",
        "def train(env, policy, optimizer, episodes=1000):\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        print(\"episode\", episode)\n",
        "        state = env.reset()\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # car pos, rock pos\n",
        "            probs = policy(state_tensor)\n",
        "            m = Categorical(probs)\n",
        "            action = m.sample()\n",
        "            state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "            log_probs.append(m.log_prob(action))\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                discounted_rewards = compute_discounted_rewards(rewards)\n",
        "                policy_loss = []\n",
        "                for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
        "                    policy_loss.append(-log_prob * Gt) # why ?\n",
        "                optimizer.zero_grad()\n",
        "                policy_loss = torch.cat(policy_loss).sum()\n",
        "                policy_loss.backward()\n",
        "                optimizer.step()  # what is this, why\n",
        "                print(\"action\", int(action))\n",
        "                print(\"Loss\", int(policy_loss))\n",
        "\n",
        "                if episode % 50 == 0:\n",
        "                    print(f\"Episode {episode}, Total Reward: {sum(rewards)}\")\n",
        "                break\n",
        "    return policy\n",
        "\n",
        "\n",
        "# %%\n",
        "# Initialize the environment and policy\n",
        "env = CarRockEnv(rock_position=[50, 50], rock_radius=5)\n",
        "policy = PolicyNetwork(input_dim=4, output_dim=4)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "train(env, policy, optimizer)\n",
        "\n",
        "evaluate_policy(policy_net=policy, env=env, episodes=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7G7oWyuEuUP0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}